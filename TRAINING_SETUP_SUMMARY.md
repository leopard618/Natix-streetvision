# ðŸš€ Natix Mining Training Setup - Complete Summary

## âœ… System Analysis Complete

### Your Hardware Specifications

| Component | Specification | Status |
|-----------|--------------|--------|
| **CPU** | AMD Ryzen Threadripper PRO 3955WX (16-core) | âœ… Excellent |
| **RAM** | 36 GB (24 GB available) | âœ… Sufficient |
| **GPU** | NVIDIA GeForce RTX 3090 | â­ **PERFECT!** |
| **VRAM** | 23.6 GB | â­ **Ideal for DINOv2-Large** |
| **CUDA** | Version 13.0 | âœ… Latest |
| **Storage** | 45 GB free | âš ï¸ **TIGHT** |
| **Python** | 3.10.12 | âœ… Compatible |

---

## â±ï¸ Expected Training Time

### DINOv2-Large Training Estimates

With your **RTX 3090 (24GB VRAM)**:

| Configuration | Time per Epoch | Total Time (100 epochs) |
|--------------|----------------|------------------------|
| **Batch size 16** | 15-20 minutes | **25-33 hours** |
| **With mixed precision (fp16)** | 12-15 minutes | **20-25 hours** |
| **With gradient accumulation** | 15-18 minutes | **25-30 hours** |

**Recommended:** Use mixed precision (fp16) for **20-25 hours total training time**

### Training Schedule Breakdown:
- **Day 1:** 0-24 hours â†’ ~80-100 epochs
- **Day 2:** Remaining epochs + evaluation
- **Total:** ~1-1.5 days of continuous training

---

## ðŸ’¾ Disk Space Requirements

### Detailed Breakdown:

| Component | Size | Location |
|-----------|------|----------|
| **CSDS Dataset** | 15-20 GB | `~/natix-mining-project/data/csds/` |
| **Natix Roadwork Dataset** | 5-10 GB | `~/natix-mining-project/data/natix/` |
| **DINOv2-Large Pre-trained** | 1.2 GB | HuggingFace cache |
| **Training Checkpoints** | 10-15 GB | `~/natix-mining-project/models/` |
| **Python Packages** | 5-8 GB | `~/natix-mining-project/venv/` |
| **Logs & Temporary** | 3-5 GB | `~/natix-mining-project/logs/` |
| **Total Required** | **45-60 GB** | |

### Your Current Status:
- **Available:** 45 GB
- **Status:** âš ï¸ **TIGHT - Need to manage carefully**

### âš ï¸ Disk Space Management Recommendations:

1. **Before training:**
   - Clean up unnecessary files: `sudo apt clean && sudo apt autoremove`
   - Remove old Docker images if any: `docker system prune -a`
   - Clear browser cache and downloads

2. **During training:**
   - Set `save_total_limit=3` (only keep 3 best checkpoints)
   - Delete intermediate checkpoints manually if needed
   - Monitor space: `watch -n 60 df -h`

3. **Alternative:** Use external storage for datasets:
   ```bash
   # Mount external drive and symlink
   ln -s /mnt/external/datasets ~/natix-mining-project/data
   ```

---

## âœ… Environment Setup Complete

### Installed Packages:

âœ… **Core ML Frameworks:**
- PyTorch 2.5.1 (with CUDA 12.1 support)
- TorchVision 0.20.1
- Transformers 5.1.0
- Datasets 4.5.0

âœ… **Training Tools:**
- Accelerate 1.12.0 (for distributed training)
- Albumentations 2.0.8 (data augmentation)
- Scikit-learn 1.7.2 (metrics)

âœ… **Experiment Tracking:**
- Weights & Biases (wandb) 0.24.2
- HuggingFace Hub 1.4.1

âœ… **Utilities:**
- Pillow 12.0.0
- NumPy 2.2.6
- Pandas 2.3.3
- tqdm 4.67.3

### Virtual Environment Location:
```
~/natix-mining-project/venv/
```

### Activation Command:
```bash
source ~/natix-mining-project/venv/bin/activate
```

---

## ðŸŽ¯ Recommended Model & Dataset

### **BEST MODEL: DINOv2-Large**

**Model:** `facebook/dinov2-large`
- **Parameters:** 304 million
- **Pre-training:** 142M images (self-supervised)
- **Architecture:** Vision Transformer
- **Why Best:** State-of-the-art foundation model, robust to variations

**Fits perfectly in your 24GB VRAM!**

### **BEST DATASETS:**

1. **CSDS Dataset** (Primary)
   - Source: `issai/CSDS_dataset` on HuggingFace
   - Size: ~15-20 GB
   - Content: Construction site satellite imagery
   - Format: YOLO-ready (easy to convert)

2. **Natix Roadwork Dataset** (Critical)
   - Source: `natix-network-org/roadwork` on HuggingFace
   - Size: ~5-10 GB
   - Content: Subnet-specific roadwork images
   - **Why Critical:** Validators use similar data

### Combined Dataset Strategy:
- **Total images:** ~30,000
- **Split:** 70% train / 15% val / 15% test
- **Classes:** Binary (roadwork vs no-roadwork)
- **Augmentation:** Heavy (weather, lighting, geometric)

---

## ðŸ“Š Expected Performance

With DINOv2-Large + CSDS + Natix datasets:

| Metric | Expected Value | Competitive Level |
|--------|---------------|-------------------|
| **Accuracy** | 93-96% | Top 10% of miners |
| **F1 Score** | 0.90-0.95 | Excellent |
| **Precision** | 0.91-0.96 | High |
| **Recall** | 0.89-0.94 | Strong |
| **Inference Time** | 50-100ms | Fast enough |

---

## ðŸš€ Next Steps

### Step 1: Verify Setup (5 minutes)
```bash
cd ~/natix-mining-project
source venv/bin/activate
python -c "import torch; print('GPU:', torch.cuda.get_device_name(0))"
```

### Step 2: Download Datasets (2-4 hours)
```bash
# Create download script
python download_datasets.py
```

### Step 3: Prepare Training Data (1-2 hours)
```bash
# Preprocess and split data
python prepare_data.py
```

### Step 4: Start Training (20-25 hours)
```bash
# Train DINOv2-Large
python train_dinov2.py
```

### Step 5: Evaluate & Upload (1 hour)
```bash
# Test model and upload to HuggingFace
python evaluate.py
python upload_to_hf.py
```

---

## ðŸ’¡ Training Optimization Tips

### For Your RTX 3090:

1. **Batch Size:** Start with 16, can try 20-24
2. **Mixed Precision:** Always use `fp16=True`
3. **Gradient Accumulation:** Use 4 steps (effective batch = 64)
4. **Learning Rate:** 5e-6 (lower for foundation models)
5. **Warmup:** 10% of total steps
6. **Scheduler:** Cosine annealing

### Monitoring:
```bash
# Terminal 1: Training
python train_dinov2.py

# Terminal 2: GPU monitoring
watch -n 1 nvidia-smi

# Terminal 3: Disk space monitoring
watch -n 60 df -h
```

---

## ðŸŽ“ Training Configuration Summary

```python
# Optimal settings for your RTX 3090
training_args = TrainingArguments(
    output_dir="./dinov2_roadwork",
    num_train_epochs=100,
    per_device_train_batch_size=16,      # Fits in 24GB VRAM
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=4,        # Effective batch = 64
    learning_rate=5e-6,                   # Low for fine-tuning
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    weight_decay=0.01,
    fp16=True,                            # Mixed precision
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    save_total_limit=3,                   # Save space!
    logging_steps=50,
    report_to="wandb",
    seed=42,
)
```

---

## âš ï¸ Important Warnings

1. **Disk Space:** Monitor constantly during training
2. **Power:** Ensure stable power supply (20-25 hours continuous)
3. **Cooling:** RTX 3090 will run hot - ensure good ventilation
4. **Backup:** Save checkpoints to external storage periodically
5. **Internet:** Stable connection needed for dataset download

---

## ðŸ“ž Troubleshooting

### Out of Memory (OOM):
```python
# Reduce batch size
per_device_train_batch_size=12  # Instead of 16
```

### Disk Space Full:
```bash
# Delete old checkpoints
rm -rf ~/natix-mining-project/models/checkpoint-*
# Keep only best model
```

### Slow Training:
```python
# Ensure fp16 is enabled
fp16=True
# Check GPU utilization
nvidia-smi
```

---

## ðŸ† Success Criteria

Your model is ready for deployment when:

âœ… Validation accuracy > 93%
âœ… F1 score > 0.90
âœ… No overfitting (train/val gap < 3%)
âœ… Inference time < 100ms per image
âœ… Model uploaded to HuggingFace with model_card.json

---

## ðŸ“ Project Structure

```
~/natix-mining-project/
â”œâ”€â”€ venv/                           # Virtual environment âœ…
â”œâ”€â”€ data/                           # Datasets (to download)
â”‚   â”œâ”€â”€ csds/                      # CSDS dataset
â”‚   â”œâ”€â”€ natix/                     # Natix roadwork
â”‚   â””â”€â”€ combined/                  # Merged & preprocessed
â”œâ”€â”€ models/                         # Saved checkpoints
â”‚   â”œâ”€â”€ dinov2_roadwork_final/    # Best model
â”‚   â””â”€â”€ checkpoints/               # Training checkpoints
â”œâ”€â”€ logs/                           # Training logs
â”‚   â”œâ”€â”€ wandb/                     # WandB logs
â”‚   â””â”€â”€ tensorboard/               # TensorBoard logs
â”œâ”€â”€ scripts/                        # Training scripts
â”‚   â”œâ”€â”€ download_datasets.py
â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â”œâ”€â”€ train_dinov2.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ upload_to_hf.py
â”œâ”€â”€ venv/                           # Virtual environment
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ðŸŽ¯ Timeline Summary

| Phase | Duration | Activity |
|-------|----------|----------|
| **Setup** | âœ… Complete | Environment & dependencies |
| **Download** | 2-4 hours | CSDS + Natix datasets |
| **Preparation** | 1-2 hours | Data preprocessing |
| **Training** | 20-25 hours | DINOv2-Large fine-tuning |
| **Evaluation** | 1 hour | Testing & metrics |
| **Upload** | 30 mins | HuggingFace deployment |
| **Total** | **~25-33 hours** | End-to-end |

---

## âœ… Setup Status

- [x] Python 3.10.12 installed
- [x] Virtual environment created
- [x] PyTorch 2.5.1 + CUDA 12.1 installed
- [x] All dependencies installed
- [x] GPU verified (RTX 3090 24GB)
- [x] 45 GB disk space available
- [ ] Datasets downloaded (next step)
- [ ] Training scripts created (next step)
- [ ] Model training (next step)

---

## ðŸš€ Ready to Start!

Your system is **perfectly configured** for high-accuracy mining on Natix subnet!

**Your RTX 3090 is ideal for this task** - it has enough VRAM for DINOv2-Large and will train efficiently.

**Next command:**
```bash
cd ~/natix-mining-project
source venv/bin/activate
# Then create and run download_datasets.py
```

Good luck with your mining! ðŸŽ‰
